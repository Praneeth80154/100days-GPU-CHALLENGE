{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f40pYSMZCKzZ",
        "outputId": "e9830aa8-5fa6-4d17-d35b-b06d266f07c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting vector_add.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile vector_add.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <stdio.h> // Include stdio.h for printf\n",
        "#include <math.h> // Include math.h for ceil\n",
        "\n",
        "__global__ void vectorAdd(const float* A, const float* B, float* C, int N) {\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (i < N) {\n",
        "        C[i] = A[i] + B[i];\n",
        "        printf(\"Thread %d: A[%d] = %f, B[%d] = %f, C[%d] = %f\\n\", i, i, A[i], i, B[i], i, C[i]);\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int N = 10;\n",
        "    float A[N], B[N], C[N];\n",
        "\n",
        "    // Initialize host arrays\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        A[i] = i;\n",
        "        B[i] = i * 2;\n",
        "    }\n",
        "\n",
        "    float *d_a, *d_b,*d_c;\n",
        "    cudaMalloc(&d_a,N*sizeof(float));\n",
        "    cudaMalloc(&d_b,N*sizeof(float));\n",
        "    cudaMalloc(&d_c,N*sizeof(float));\n",
        "    cudaMemcpy(d_a,A,N*sizeof(float),cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b,B,N*sizeof(float),cudaMemcpyHostToDevice);\n",
        "\n",
        "    int blocksize=256;\n",
        "    int gridsize=ceil((float)N/blocksize); // Cast N to float for ceil\n",
        "\n",
        "    printf(\"Launching kernel with grid size %d and block size %d\\n\", gridsize, blocksize);\n",
        "    vectorAdd<<<gridsize,blocksize>>>(d_a,d_b,d_c,N);\n",
        "    cudaDeviceSynchronize(); // Synchronize to ensure all kernel threads complete\n",
        "    cudaMemcpy(C,d_c,N*sizeof(float),cudaMemcpyDeviceToHost);\n",
        "\n",
        "    printf(\"\\nResult on host:\\n\");\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        printf(\"C[%d] = %f\\n\", i, C[i]);\n",
        "    }\n",
        "\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile with the specified architecture\n",
        "!nvcc vector_add.cu -o vector_add -gencode arch=compute_75,code=sm_75\n",
        "\n",
        "# Run the executable\n",
        "!./vector_add"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thEVXDprCYT4",
        "outputId": "d25b1f6c-43d6-4858-e08f-03f4fa4007f9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Launching kernel with grid size 1 and block size 256\n",
            "\n",
            "Result on host:\n",
            "C[0] = 0.000000\n",
            "C[1] = 0.000000\n",
            "C[2] = 0.000000\n",
            "C[3] = 0.000000\n",
            "C[4] = 0.000000\n",
            "C[5] = 0.000000\n",
            "C[6] = 0.000000\n",
            "C[7] = 0.000000\n",
            "C[8] = 0.000000\n",
            "C[9] = 0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b2mX6A0UCvT5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}